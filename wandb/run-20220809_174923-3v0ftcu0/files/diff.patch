diff --git a/examples/dreamer/config.yaml b/examples/dreamer/config.yaml
index 850c199..8f8e84c 100644
--- a/examples/dreamer/config.yaml
+++ b/examples/dreamer/config.yaml
@@ -1,27 +1,25 @@
 env_name: cheetah
 env_task: run
 env_library: dm-control
-async_collection: 0
-record_video: 1
-normalize_rewards_online: 1
-normalize_rewards_online_scale: 5
-frame_skip: 1
+async_collection: 1
+record_video: 0
+frame_skip: 2
 frames_per_batch: 1000
-optim_steps_per_batch: 10
-batch_size: 256
+optim_steps_per_batch: 100
+batch_size: 50
 total_frames: 5000000
 world_model_lr: 6e-4
-lr: 8e-5
-from_pixels: 1
+actor_value_lr: 8e-5
+from_pixels: True
 #collector_devices: [cuda:1]
 collector_devices: [cpu]
 env_per_collector: 4
 num_workers: 4
-lr_scheduler: ""
 record_interval: 100
 max_frames_per_traj: -1
-weight_decay: 0.0
-init_env_steps: 10000
+init_env_steps: 25000
 record_frames: 50000
-loss_function: smooth_l1
 batch_transform: 1
+state_dim: 20
+rssm_hidden_dim: 200
+grad_clip: 100
diff --git a/examples/dreamer/dreamer.py b/examples/dreamer/dreamer.py
index cd58e6b..84eff0f 100644
--- a/examples/dreamer/dreamer.py
+++ b/examples/dreamer/dreamer.py
@@ -1,4 +1,5 @@
 import dataclasses
+from dataclasses import dataclass
 import uuid
 from datetime import datetime
 
@@ -8,11 +9,11 @@ from hydra.core.config_store import ConfigStore
 from torchrl.envs import ParallelEnv, EnvCreator
 from torchrl.envs.transforms import RewardScaling, TransformedEnv
 from torchrl.envs.model_based import ModelBasedEnv
-from torchrl.modules import EGreedyWrapper
 from torchrl.modules.tensordict_module.actors import (
     ActorCriticWrapper,
     WorldModelWrapper,
 )
+from torchrl.modules.tensordict_module.sequence import TensorDictSequence
 from torchrl.record import VideoRecorder
 from torchrl.trainers.helpers.collectors import (
     make_collector_offpolicy,
@@ -34,18 +35,28 @@ from torchrl.trainers.helpers.replay_buffer import (
     make_replay_buffer,
     ReplayArgsConfig,
 )
-
+import tqdm
 import torch.nn as nn
+import torch
+from torch.nn.utils import clip_grad_norm_
 
-DEFAULT_REWARD_SCALING = {
-    "Hopper-v1": 5,
-    "Walker2d-v1": 5,
-    "HalfCheetah-v1": 5,
-    "cheetah": 5,
-    "Ant-v2": 5,
-    "Humanoid-v2": 20,
-    "humanoid": 100,
-}
+@dataclass
+class DreamerConfig:
+    state_dim: int = 20
+    rssm_hidden_dim: int = 200
+    grad_clip: int = 100
+    world_model_lr: float = 6e-4
+    actor_value_lr: float = 8e-5
+
+@dataclass
+class TrainingConfig:
+    optim_steps_per_batch: int = 500
+    # Number of optimization steps in between two collection of data. See frames_per_batch below.
+    # LR scheduler.
+    batch_size: int = 256
+    # batch size of the TensorDict retrieved from the replay buffer. Default=256.
+    log_interval: int = 10000
+    # logging interval, in terms of optimization steps. Default=10000.
 
 config_fields = [
     (config_field.name, config_field.type, config_field)
@@ -54,6 +65,9 @@ config_fields = [
         EnvConfig,
         RecorderConfig,
         ReplayArgsConfig,
+        DreamerConfig,
+        TrainingConfig
+
     )
     for config_field in dataclasses.fields(config_cls)
 ]
@@ -61,6 +75,16 @@ Config = dataclasses.make_dataclass(cls_name="Config", fields=config_fields)
 cs = ConfigStore.instance()
 cs.store(name="config", node=Config)
 
+DEFAULT_REWARD_SCALING = {
+    "Hopper-v1": 5,
+    "Walker2d-v1": 5,
+    "HalfCheetah-v1": 5,
+    "cheetah": 5,
+    "Ant-v2": 5,
+    "Humanoid-v2": 20,
+    "humanoid": 100,
+}
+
 
 @hydra.main(version_base=None, config_path=".", config_name="config")
 def main(cfg: "DictConfig"):
@@ -107,8 +131,8 @@ def main(cfg: "DictConfig"):
         cfg=cfg, use_env_creator=False, stats=stats
     )()
 
-    #### Model Based model and loss
-    world_modeler = DreamerWorldModeler()
+    #### World Model and reward model
+    world_modeler = DreamerWorldModeler(rssm_hidden=cfg.rssm_hidden_dim, rnn_hidden_dim=cfg.rssm_hidden_dim, state_dim=cfg.state_dim)
     reward_model = TensorDictModule(
         MLP(out_features=1, depth=3, num_cells=300, activation_class=nn.ELU),
         in_keys=["posterior_state", "belief"],
@@ -118,20 +142,18 @@ def main(cfg: "DictConfig"):
     model_based_env = ModelBasedEnv(
         world_model=WorldModelWrapper(
             world_modeler.select_subsequence(
-                in_keys=["initial_state", "initial_rnn_hidden", "action"],
-                out_keys=["prior_states", "prior_rnn_hiddens"],
+                in_keys=["prior_state", "belief", "action"],
+                out_keys=["next_prior_states", "next_belief"],
             ),
             TensorDictModule(
                 reward_model.module,
-                in_keys=["prior_states", "belief"],
+                in_keys=["next_prior_states", "next_belief"],
                 out_keys=["predicted_reward"],
             ),
         ),
         device=device,
     )
 
-    world_model_loss = DreamerModelLoss()
-    world_model_opt = torch.optim.Adam(world_model.parameters(), lr=cfg.lr)
     ### Actor and Value models
     actor_model = TensorDictModule(
         TanhActor(
@@ -140,24 +162,35 @@ def main(cfg: "DictConfig"):
             num_cells=300,
             activation_class=nn.ELU,
         ),
-        in_keys=["posterior_state", "belief"],
+        in_keys=["prior_state", "belief"],
         out_keys=["action"],
     )
     value_model = TensorDictModule(
         MLP(out_features=1, depth=3, num_cells=400, activation_class=nn.ELU),
-        in_keys=["posterior_state", "belief"],
+        in_keys=["prior_state", "belief"],
         out_keys=["predicted_value"],
     )
     actor_value_model = ActorCriticWrapper(actor_model, value_model)
-    behaviour_loss = DreamerBehaviourLoss()
-    #### Off Policy Collector
-    collector = make_collector_offpolicy(
-        cfg=cfg,
-        env=model_based_env,
-        device=device,
-        logger=logger,
-        video_tag=video_tag,
+
+    ### Policy to compute inference from observations
+    policy = TensorDictSequence(
+        world_modeler.select_subsequence(
+            out_keys=["posterior_state", "belief"],
+        ),
+        TensorDictModule(
+            actor_model.module,
+            in_keys=["prior_states", "belief"],
+            out_keys=["action"],
+        ),
     )
+    #### Losses
+    behaviour_loss = DreamerBehaviourLoss()
+    world_model_loss = DreamerModelLoss()
+
+    ### optimizers
+    world_model_opt = torch.optim.Adam(world_model.parameters(), lr=cfg.world_model_lr)
+    actor_opt = torch.optim.Adam(actor_model.parameters(), lr=cfg.actor_value_lr)
+    value_opt = torch.optim.Adam(value_model.parameters(), lr=cfg.actor_value_lr)
 
     #### Recorder
     if cfg.record_video:
@@ -171,28 +204,8 @@ def main(cfg: "DictConfig"):
     else:
         recorder = None
 
-    #### Replay Buffer
-    replay_buffer = make_replay_buffer(
-        cfg=cfg, env=proof_env, device=device, logger=logger
-    )
-
-    #### Training Loop
-    collector.train(
-        replay_buffer=replay_buffer,
-        recorder=recorder,
-        num_episodes=cfg.num_episodes,
-        num_steps=cfg.num_steps,
-        num_steps_per_episode=cfg.num_steps_per_episode,
-        num_steps_per_eval=cfg.num_steps_per_eval,
-        num_steps_per_video=cfg.num_steps_per_video,
-        num_steps_per_log=cfg.num_steps_per_log,
-        num_steps_per_save=cfg.num_steps_per_save,
-    )
-
     #### Actor and value network
-    model_explore = EGreedyWrapper(model, annealing_num_steps=cfg.annealing_frames).to(
-        device
-    )
+    model_explore = policy
 
     action_dim_gsde, state_dim_gsde = None, None
     proof_env.close()
@@ -245,23 +258,79 @@ def main(cfg: "DictConfig"):
             t.scale.fill_(1.0)
             t.loc.fill_(0.0)
 
-    trainer = make_trainer(
-        collector,
-        loss_module,
-        recorder,
-        target_net_updater,
-        model,
-        replay_buffer,
-        logger,
-        cfg,
-    )
+    # trainer = make_trainer(
+    #     collector,
+    #     loss_module,
+    #     recorder,
+    #     target_net_updater,
+    #     model,
+    #     replay_buffer,
+    #     logger,
+    #     cfg,
+    # )
 
     final_seed = collector.set_seed(cfg.seed)
     print(f"init seed: {cfg.seed}, final seed: {final_seed}")
     ## Training loop
-    trainer.train()
-    return (logger.log_dir, trainer._log_dict)
+    collected_frames = 0
+    pbar = tqdm.tqdm(total=cfg.total_frames//cfg.frame_skip)
+    r0 = None
+    for i, tensordict in enumerate(collector):
+
+        # update weights of the inference policy
+        collector.update_policy_weights_()
+        
+        if r0 is None:
+            r0 = tensordict["reward"].mean().item()
+        pbar.update(tensordict.numel())
+        
+        # extend the replay buffer with the new data
+        if "mask" in tensordict.keys():
+            # if multi-step, a mask is present to help filter padded values
+            current_frames = tensordict["mask"].sum()
+            tensordict = tensordict[tensordict.get("mask").squeeze(-1)]
+        else:
+            tensordict = tensordict.view(-1)
+            current_frames = tensordict.numel()
+        collected_frames += current_frames
+        replay_buffer.extend(tensordict.cpu())
+
+        # optimization steps
+        if collected_frames >= cfg.init_env_steps:
+            for j in range(cfg.optim_steps_per_batch):
+                # sample from replay buffer
+                sampled_tensordict = replay_buffer.sample(cfg.batch_size)
+                sampled_tensordict.batch_size = [sampled_tensordict.shape[0]]
+                sampled_tensordict["initial_state"] = torch.zeros((sampled_tensordict.batch_size[0],1,cfg.state_dim))
+                sampled_tensordict["initial_belief"] = torch.zeros((sampled_tensordict.batch_size[0],1,cfg.rssm_hidden_dim))
+                world_model.train()
+                sampled_tensordict = world_model(sampled_tensordict)
+                # compute model loss
+                model_loss_td = world_model_loss(sampled_tensordict)
+                world_model_opt.zero_grad()
+                model_loss_td["loss"].backward()
+                clip_grad_norm_(world_model.parameters(), cfg.grad_clip)
+                world_model_opt.step()
 
+                flattened_td = sampled_tensordict.select("prior_state", "belief").view(-1, sampled_tensordict.shape[-1]).detach()
+                with torch.no_grad:
+                    flattened_td = model_based_env.rollout(flattened_td)
+                flattened_td = actor_value_model(flattened_td)
+                # compute actor loss
+                actor_value_loss_td = behaviour_loss(flattened_td)
+                actor_opt.zero_grad()
+                actor_value_loss_td["loss_actor"].backward()
+                clip_grad_norm_(actor_model.parameters(), cfg.grad_clip)
+                actor_opt.step()
+
+                # Optimize value function
+                value_opt.zero_grad()
+                actor_value_loss_td["loss_value"].backward()
+                clip_grad_norm_(value_model.parameters(), cfg.grad_clip)
+                value_opt.step()
+            td_record = recorder(None)
 
 if __name__ == "__main__":
     main()
+
+
diff --git a/torchrl/modules/tensordict_module/world_models.py b/torchrl/modules/tensordict_module/world_models.py
index 8437020..2354520 100644
--- a/torchrl/modules/tensordict_module/world_models.py
+++ b/torchrl/modules/tensordict_module/world_models.py
@@ -29,8 +29,8 @@ class DreamerWorldModeler(TensorDictSequence):
                     rnn_hidden_dim=rnn_hidden_dim,
                     state_dim=state_dim,
                 ),
-                in_keys=["initial_state", "initial_rnn_hidden", "action"],
-                out_keys=["prior_mean", "prior_std", "prior_state", "belief"],
+                in_keys=["prior_state", "initial_belief", "action"],
+                out_keys=["next_prior_mean", "next_prior_std", "next_prior_state", "next_belief"],
             ),
             TensorDictModule(
                 RSSMPosterior(
