diff --git a/examples/dreamer/dreamer.py b/examples/dreamer/dreamer.py
index 84eff0f..bd83783 100644
--- a/examples/dreamer/dreamer.py
+++ b/examples/dreamer/dreamer.py
@@ -47,6 +47,7 @@ class DreamerConfig:
     grad_clip: int = 100
     world_model_lr: float = 6e-4
     actor_value_lr: float = 8e-5
+    imagination_horizon: int = 15
 
 @dataclass
 class TrainingConfig:
@@ -143,11 +144,11 @@ def main(cfg: "DictConfig"):
         world_model=WorldModelWrapper(
             world_modeler.select_subsequence(
                 in_keys=["prior_state", "belief", "action"],
-                out_keys=["next_prior_states", "next_belief"],
+                out_keys=["next_prior_state", "next_belief"],
             ),
             TensorDictModule(
                 reward_model.module,
-                in_keys=["next_prior_states", "next_belief"],
+                in_keys=["next_prior_state", "next_belief"],
                 out_keys=["predicted_reward"],
             ),
         ),
@@ -179,7 +180,7 @@ def main(cfg: "DictConfig"):
         ),
         TensorDictModule(
             actor_model.module,
-            in_keys=["prior_states", "belief"],
+            in_keys=["posterior_state", "belief"],
             out_keys=["action"],
         ),
     )
@@ -301,8 +302,8 @@ def main(cfg: "DictConfig"):
                 # sample from replay buffer
                 sampled_tensordict = replay_buffer.sample(cfg.batch_size)
                 sampled_tensordict.batch_size = [sampled_tensordict.shape[0]]
-                sampled_tensordict["initial_state"] = torch.zeros((sampled_tensordict.batch_size[0],1,cfg.state_dim))
-                sampled_tensordict["initial_belief"] = torch.zeros((sampled_tensordict.batch_size[0],1,cfg.rssm_hidden_dim))
+                sampled_tensordict["prior_state"] = torch.zeros((sampled_tensordict.batch_size[0],1,cfg.state_dim))
+                sampled_tensordict["belief"] = torch.zeros((sampled_tensordict.batch_size[0],1,cfg.rssm_hidden_dim))
                 world_model.train()
                 sampled_tensordict = world_model(sampled_tensordict)
                 # compute model loss
@@ -313,8 +314,9 @@ def main(cfg: "DictConfig"):
                 world_model_opt.step()
 
                 flattened_td = sampled_tensordict.select("prior_state", "belief").view(-1, sampled_tensordict.shape[-1]).detach()
+                flattened_td = actor_model(flattened_td)
                 with torch.no_grad:
-                    flattened_td = model_based_env.rollout(flattened_td)
+                    flattened_td = model_based_env.rollout(max_steps=cfg.imagination_horizon, policy=actor_model, auto_reset=False, tensordict=flattened_td)
                 flattened_td = actor_value_model(flattened_td)
                 # compute actor loss
                 actor_value_loss_td = behaviour_loss(flattened_td)
diff --git a/torchrl/envs/model_based.py b/torchrl/envs/model_based.py
index c4f2df6..760a795 100644
--- a/torchrl/envs/model_based.py
+++ b/torchrl/envs/model_based.py
@@ -91,13 +91,11 @@ class ModelBasedEnv(EnvBase, metaclass=abc.ABCMeta):
         tensordict_out["done"] = torch.zeros(tensordict_out.shape, dtype=torch.bool)
         return tensordict_out
     
-    @abc.abstractmethod
     def _reset(self, tensordict: TensorDict, **kwargs) -> TensorDict:
-        raise NotImplementedError
+        return tensordict.clone()
     
-    @abc.abstractmethod
     def _set_seed(self, seed: Optional[int]) -> int:
-        raise NotImplementedError
+        return seed
 
     def to(self, device: DEVICE_TYPING) -> ModelBasedEnv:
         super().to(device)
diff --git a/torchrl/modules/tensordict_module/world_models.py b/torchrl/modules/tensordict_module/world_models.py
index 2354520..b955f98 100644
--- a/torchrl/modules/tensordict_module/world_models.py
+++ b/torchrl/modules/tensordict_module/world_models.py
@@ -29,7 +29,7 @@ class DreamerWorldModeler(TensorDictSequence):
                     rnn_hidden_dim=rnn_hidden_dim,
                     state_dim=state_dim,
                 ),
-                in_keys=["prior_state", "initial_belief", "action"],
+                in_keys=["prior_state", "belief", "action"],
                 out_keys=["next_prior_mean", "next_prior_std", "next_prior_state", "next_belief"],
             ),
             TensorDictModule(
@@ -37,7 +37,7 @@ class DreamerWorldModeler(TensorDictSequence):
                     hidden_dim=rssm_hidden,
                     state_dim=state_dim,
                 ),
-                in_keys=["action", "observation_encoded"],
+                in_keys=["belief", "observation_encoded"],
                 out_keys=["posterior_mean", "posterior_std", "posterior_state"],
             ),
             TensorDictModule(
